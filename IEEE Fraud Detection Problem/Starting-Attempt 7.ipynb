{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn import ensemble,metrics,cross_decomposition,linear_model,model_selection,preprocessing\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIdentity=pd.read_csv('train_identity.csv')\n",
    "trainTranscation=pd.read_csv('train_transaction.csv')\n",
    "trainIdentity.shape,trainTranscation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIdentity=pd.read_csv('test_identity.csv')\n",
    "testTranscation=pd.read_csv('test_transaction.csv')\n",
    "testIdentity.shape,testTranscation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedData=pd.merge(left=trainTranscation,right=trainIdentity,on='TransactionID',how='left')\n",
    "combinedDataTest=pd.merge(left=testTranscation,right=testIdentity,on='TransactionID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del testIdentity,testTranscation,trainIdentity,trainTranscation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(2826, 'card1'),\n",
    " (2481, 'card2'),\n",
    " (1865, 'addr1'),\n",
    " (1801, 'TransactionAmt'),\n",
    " (1425, 'C13'),\n",
    " (1198, 'D15'),\n",
    " (1162, 'D2'),\n",
    " (1097, 'C1'),\n",
    " (1056, 'P_emaildomain'),\n",
    " (1024, 'D1'),\n",
    " (982, 'D10'),\n",
    " (977, 'card5'),\n",
    " (958, 'D4'),\n",
    " (884, 'transDay'),\n",
    " (851, 'dist1'),\n",
    " (842, 'C14'),\n",
    " (796, 'C11'),\n",
    " (794, 'transMonth'),\n",
    " (766, 'C2'),\n",
    " (753, 'D8')]\n",
    "\n",
    "impVar=['card1', 'card2', 'addr1', 'TransactionAmt', 'C13', 'D15', 'D2', 'C1', 'P_emaildomain', 'D1', 'D10', 'card5', 'D4', 'transDay', 'dist1', 'C14', 'C11', 'transMonth', 'C2', 'D8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDataTest['diffName']='test'\n",
    "combinedData['diffName']='train'\n",
    "\n",
    "newSetData=pd.concat([combinedData,combinedDataTest],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cardVsTransAmTest1=combinedData.groupby('card1').agg({'TransactionAmt':{'Card1VsTrans':'mean'}})\n",
    "cardVsTransAmTest1.columns = cardVsTransAmTest1.columns.droplevel(level=0)\n",
    "cardVsTransAmTest1=cardVsTransAmTest1.reset_index()\n",
    "cardVsTransAmTest1Dict={cardVsTransAmTest1['card1'].iloc[i]:cardVsTransAmTest1['Card1VsTrans'].iloc[i] for i in range(cardVsTransAmTest1.shape[0])}\n",
    "\n",
    "cardVsTransAmTest1Std=combinedData.groupby('card1').agg({'TransactionAmt':{'Card1VsTransStd':'std'}})\n",
    "cardVsTransAmTest1Std.columns = cardVsTransAmTest1Std.columns.droplevel(level=0)\n",
    "cardVsTransAmTest1Std=cardVsTransAmTest1Std.reset_index()\n",
    "cardVsTransAmTest1StdDict={cardVsTransAmTest1Std['card1'].iloc[i]:cardVsTransAmTest1Std['Card1VsTransStd'].iloc[i] for i in range(cardVsTransAmTest1Std.shape[0])}\n",
    "\n",
    "cardVsTransAmTest4=combinedData.groupby('card4').agg({'TransactionAmt':{'Card4VsTrans':'mean'}})\n",
    "cardVsTransAmTest4.columns = cardVsTransAmTest4.columns.droplevel(level=0)\n",
    "cardVsTransAmTest4=cardVsTransAmTest4.reset_index()\n",
    "cardVsTransAmTest4Dict={cardVsTransAmTest4['card4'].iloc[i]:cardVsTransAmTest4['Card4VsTrans'].iloc[i] for i in range(cardVsTransAmTest4.shape[0])}\n",
    "\n",
    "cardVsTransAmTest4Std=combinedData.groupby('card4').agg({'TransactionAmt':{'Card4VsTransStd':'std'}})\n",
    "cardVsTransAmTest4Std.columns = cardVsTransAmTest4Std.columns.droplevel(level=0)\n",
    "cardVsTransAmTest4Std=cardVsTransAmTest4Std.reset_index()\n",
    "cardVsTransAmTest4StdDict={cardVsTransAmTest4Std['card4'].iloc[i]:cardVsTransAmTest4Std['Card4VsTransStd'].iloc[i] for i in range(cardVsTransAmTest4Std.shape[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cardVsTransAmTest1Dict),len(cardVsTransAmTest1StdDict),len(cardVsTransAmTest4Dict),len(cardVsTransAmTest4StdDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browserDef(brRec):\n",
    "    if str(brRec)=='nan':\n",
    "        return 'other'\n",
    "    else:\n",
    "        if 'samsung' in brRec:\n",
    "            return 'Samsung Browser'\n",
    "        elif 'mobile safari' in brRec:\n",
    "            return 'Mobile Safari'\n",
    "        elif 'chrome' in brRec:\n",
    "            return 'Chrome Browser'\n",
    "        elif 'edge' in brRec:\n",
    "            return 'Edge Browser'\n",
    "        elif 'ie' in brRec:\n",
    "            return 'IE Browser'\n",
    "        elif 'firefox' in brRec:\n",
    "            return 'Firefox Browser'\n",
    "        elif 'opera' in brRec:\n",
    "            return 'Opera Browser'\n",
    "        elif ('Android' in brRec) or ('android' in brRec):\n",
    "            return 'Android Browser'\n",
    "        elif 'Mozilla' in brRec:\n",
    "            return 'Mozilla Browser'\n",
    "        elif 'safari' in brRec:\n",
    "            return 'Safari  Browser'\n",
    "        elif 'google' in brRec:\n",
    "            return 'Google Browser'\n",
    "        else:\n",
    "            return brRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviceDef(brRec):\n",
    "    if str(brRec)=='nan':\n",
    "        return 'other'\n",
    "    else:\n",
    "        if 'Android' in brRec:\n",
    "            return 'Android Device'\n",
    "        elif 'iOS' in brRec:\n",
    "            return 'iOS Device'\n",
    "        elif 'Windows' in brRec:\n",
    "            return 'Windows Device'\n",
    "        elif 'Mac' in brRec:\n",
    "            return 'Mac OS Device'\n",
    "        else:\n",
    "            return brRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def screenReso(xRes):\n",
    "    if str(xRes) == 'nan':\n",
    "        return ('No info')\n",
    "    else:\n",
    "        widVal=int(xRes.split('x')[0])\n",
    "        if widVal <=850:\n",
    "            return ('Small Screen')\n",
    "        elif widVal <=2050:\n",
    "            return ('Med Screen')\n",
    "        elif widVal <=2250:\n",
    "            return ('2K Screen')\n",
    "        elif widVal > 2250:\n",
    "            return ('4K Screen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defGetCountryFromDomain(brRec):\n",
    "    if str(brRec)=='nan':\n",
    "        return 'Other'\n",
    "    else:\n",
    "        if ('.mx' in brRec):\n",
    "            return 'Mexico email'\n",
    "        elif '.jp' in brRec:\n",
    "            return 'Japan email'\n",
    "        elif '.uk' in brRec:\n",
    "            return 'UK email'\n",
    "        elif '.de' in brRec:\n",
    "            return 'Germany email'\n",
    "        elif '.es' in brRec:\n",
    "            return 'Spain email'\n",
    "        elif '.fr' in brRec:\n",
    "            return 'France email'\n",
    "        elif '.com' in brRec:\n",
    "            return 'Global email'\n",
    "        elif '.net' in brRec:\n",
    "            return 'Net email'\n",
    "        else:\n",
    "            return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #added on 2nd trial\n",
    "# combinedData['id_33']=combinedData['id_33'].apply(lambda x: screenReso(x))\n",
    "# #added on 3rd trial\n",
    "# # combinedData['id_31']=combinedData['id_31'].apply(lambda x: browserDef(x))\n",
    "# #added on 4th trial\n",
    "# #combinedData['id_30']=combinedData['id_30'].apply(lambda x: deviceDef(x))\n",
    "\n",
    "# combinedData['CountryDomain']=combinedData['P_emaildomain'].apply(lambda x: defGetCountryFromDomain(x))\n",
    "# # combinedData['R_emaildomain']=combinedData['R_emaildomain'].fillna('Other')\n",
    "# # combinedData['P_emaildomain']=combinedData['P_emaildomain'].fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defCard6(brRec):\n",
    "    if str(brRec)=='nan':\n",
    "        return 'Other'\n",
    "    else:\n",
    "        if ('debit or credit' in brRec):\n",
    "            return 'debit'\n",
    "        else:\n",
    "            return (brRec)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newSetData['id_33']=newSetData['id_33'].apply(lambda x: screenReso(x))\n",
    "newSetData['id_31']=newSetData['id_31'].apply(lambda x: browserDef(x))\n",
    "newSetData['id_30']=newSetData['id_30'].apply(lambda x: deviceDef(x))\n",
    "newSetData['card6']=newSetData['card6'].apply(lambda x: defCard6(x))\n",
    "newSetData['CountryDomain']=newSetData['P_emaildomain'].apply(lambda x: defGetCountryFromDomain(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalCola=['id_33','id_31','id_30','card6','CountryDomain']\n",
    "\n",
    "getDummVar=pd.get_dummies(newSetData,columns=categoricalCola)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nP=np.percentile(getDummVar['TransactionAmt'],99)\n",
    "getDummVar['TransactionAmt']=getDummVar['TransactionAmt'].apply(lambda x: nP if x >= nP else x)\n",
    "# sns.distplot(combinedData['TransactionAmt'])\n",
    "getDummVar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDummVar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempTimeSer=((getDummVar['TransactionDT']-86400)).map(int)\n",
    "getDummVar['transSec']=tempTimeSer%60\n",
    "getDummVar['transMin']=(tempTimeSer/60).map(int)%60\n",
    "getDummVar['transHour']=(tempTimeSer/3600).map(int)%24\n",
    "getDummVar['transMonth']=((tempTimeSer/86400)//30)+1\n",
    "getDummVar['transDay']=((tempTimeSer/86400)%30).map(int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del getDummVar['TransactionDT']\n",
    "del getDummVar['TransactionID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDummVar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPart=getDummVar[getDummVar['diffName']=='train']\n",
    "testDataPart=getDummVar[getDummVar['diffName']=='test']\n",
    "del testDataPart['isFraud']\n",
    "del getDummVar\n",
    "del newSetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainDataPart['diffName']\n",
    "del testDataPart['diffName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = trainDataPart['isFraud'].copy()\n",
    "# del train_transaction, train_identity, test_transaction, test_identity\n",
    "\n",
    "# Drop target, fill in NaNs\n",
    "X_train = trainDataPart.drop('isFraud', axis=1)\n",
    "X_test = testDataPart.copy()\n",
    "\n",
    "del trainDataPart, testDataPart\n",
    "\n",
    "X_train = X_train.fillna(-999)\n",
    "X_test = X_test.fillna(-999)\n",
    "\n",
    "# Label Encoding\n",
    "for f in X_train.columns:\n",
    "    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n",
    "        X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "        X_test[f] = lbl.transform(list(X_test[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape\n",
    "#((590540, 436), (506691, 436))\n",
    "#((590540, 436), (506691, 436))\n",
    "#((590540, 449), (506691, 450))\n",
    "#((590540, 496), (506691, 496))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in list(X_test.columns)  if i not in list(X_train.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['isFraud']=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPart=X_train[X_train['transMonth']<=4]\n",
    "valPart=X_train[X_train['transMonth']==5]\n",
    "testPart=X_train[X_train['transMonth']>5]\n",
    "trainPart.shape,valPart.shape,testPart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((pd.unique(trainPart['C1'])).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainPart[impVar].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For train Val part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardVsTransAmtrainPart=trainPart.groupby('card1').agg({'TransactionAmt':{'Card1VsTrans':'mean'}})\n",
    "# cardVsTransAmtrainPart.columns = cardVsTransAmtrainPart.columns.droplevel(level=0)\n",
    "# cardVsTransAmtrainPart=cardVsTransAmtrainPart.reset_index()\n",
    "# cardVsTransAmtrainPartDict={cardVsTransAmtrainPart['card1'].iloc[i]:cardVsTransAmtrainPart['Card1VsTrans'].iloc[i] for i in range(cardVsTransAmtrainPart.shape[0])}\n",
    "\n",
    "# cardVsTransAmtrainPartStd1=trainPart.groupby('card1').agg({'TransactionAmt':{'Card1VsTransStd':'std'}})\n",
    "# cardVsTransAmtrainPartStd1.columns = cardVsTransAmtrainPartStd1.columns.droplevel(level=0)\n",
    "# cardVsTransAmtrainPartStd1=cardVsTransAmtrainPartStd1.reset_index()\n",
    "# cardVsTransAmtrainPartStd1Dict={cardVsTransAmtrainPartStd1['card1'].iloc[i]:cardVsTransAmtrainPartStd1['Card1VsTransStd'].iloc[i] for i in range(cardVsTransAmtrainPartStd1.shape[0])}\n",
    "\n",
    "# cardVsTransAmtrainPart4=trainPart.groupby('card4').agg({'TransactionAmt':{'Card4VsTrans':'mean'}})\n",
    "# cardVsTransAmtrainPart4.columns = cardVsTransAmtrainPart4.columns.droplevel(level=0)\n",
    "# cardVsTransAmtrainPart4=cardVsTransAmtrainPart4.reset_index()\n",
    "# cardVsTransAmtrainPart4Dict={cardVsTransAmtrainPart4['card4'].iloc[i]:cardVsTransAmtrainPart4['Card4VsTrans'].iloc[i] for i in range(cardVsTransAmtrainPart4.shape[0])}\n",
    "\n",
    "# cardVsTransAmtrainPart4Std4=trainPart.groupby('card4').agg({'TransactionAmt':{'Card4VsTransStd':'std'}})\n",
    "# cardVsTransAmtrainPart4Std4.columns = cardVsTransAmtrainPart4Std4.columns.droplevel(level=0)\n",
    "# cardVsTransAmtrainPart4Std4=cardVsTransAmtrainPart4Std4.reset_index()\n",
    "# cardVsTransAmtrainPart4Std4Dict={cardVsTransAmtrainPart4Std4['card4'].iloc[i]:cardVsTransAmtrainPart4Std4['Card4VsTransStd'].iloc[i] for i in range(cardVsTransAmtrainPart4Std4.shape[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(cardVsTransAmtrainPartDict),len(cardVsTransAmtrainPartStd1Dict),len(cardVsTransAmtrainPart4Dict),len(cardVsTransAmtrainPart4Std4Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainPart['Card1VsTrans']=trainPart['card1'].map(cardVsTransAmtrainPartDict)\n",
    "# trainPart['Card1VsTransStd']=trainPart['card1'].map(cardVsTransAmtrainPartStd1Dict)\n",
    "# trainPart['Card4VsTrans']=trainPart['card4'].map(cardVsTransAmtrainPart4Dict)\n",
    "# trainPart['Card4VsTransStd']=trainPart['card4'].map(cardVsTransAmtrainPart4Std4Dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valPart['Card1VsTrans']=valPart['card1'].map(cardVsTransAmtrainPartDict)\n",
    "# valPart['Card1VsTransStd']=valPart['card1'].map(cardVsTransAmtrainPartStd1Dict)\n",
    "# valPart['Card4VsTrans']=valPart['card4'].map(cardVsTransAmtrainPart4Dict)\n",
    "# valPart['Card4VsTransStd']=valPart['card4'].map(cardVsTransAmtrainPart4Std4Dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For testpart local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forCalFeatureData=trainPart.append(valPart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardVsTransAmtestLocal1=forCalFeatureData.groupby('card1').agg({'TransactionAmt':{'Card1VsTrans':'mean'}})\n",
    "# cardVsTransAmtestLocal1.columns = cardVsTransAmtestLocal1.columns.droplevel(level=0)\n",
    "# cardVsTransAmtestLocal1=cardVsTransAmtestLocal1.reset_index()\n",
    "# cardVsTransAmtestLocal1Dict={cardVsTransAmtestLocal1['card1'].iloc[i]:cardVsTransAmtestLocal1['Card1VsTrans'].iloc[i] for i in range(cardVsTransAmtestLocal1.shape[0])}\n",
    "\n",
    "# cardVsTransAmtestLocal1Std1=forCalFeatureData.groupby('card1').agg({'TransactionAmt':{'Card1VsTransStd':'std'}})\n",
    "# cardVsTransAmtestLocal1Std1.columns = cardVsTransAmtestLocal1Std1.columns.droplevel(level=0)\n",
    "# cardVsTransAmtestLocal1Std1=cardVsTransAmtestLocal1Std1.reset_index()\n",
    "# cardVsTransAmtestLocalStd1Dict={cardVsTransAmtestLocal1Std1['card1'].iloc[i]:cardVsTransAmtestLocal1Std1['Card1VsTransStd'].iloc[i] for i in range(cardVsTransAmtestLocal1Std1.shape[0])}\n",
    "\n",
    "# cardVsTransAmtestLocal4=forCalFeatureData.groupby('card4').agg({'TransactionAmt':{'Card4VsTrans':'mean'}})\n",
    "# cardVsTransAmtestLocal4.columns = cardVsTransAmtestLocal4.columns.droplevel(level=0)\n",
    "# cardVsTransAmtestLocal4=cardVsTransAmtestLocal4.reset_index()\n",
    "# cardVsTransAmtestLocal4Dict={cardVsTransAmtestLocal4['card4'].iloc[i]:cardVsTransAmtestLocal4['Card4VsTrans'].iloc[i] for i in range(cardVsTransAmtestLocal4.shape[0])}\n",
    "\n",
    "# cardVsTransAmtestLocal4Std4=forCalFeatureData.groupby('card4').agg({'TransactionAmt':{'Card4VsTransStd':'std'}})\n",
    "# cardVsTransAmtestLocal4Std4.columns = cardVsTransAmtestLocal4Std4.columns.droplevel(level=0)\n",
    "# cardVsTransAmtestLocal4Std4=cardVsTransAmtestLocal4Std4.reset_index()\n",
    "# cardVsTransAmtrainPart4Std4Dict={cardVsTransAmtestLocal4Std4['card4'].iloc[i]:cardVsTransAmtestLocal4Std4['Card4VsTransStd'].iloc[i] for i in range(cardVsTransAmtestLocal4Std4.shape[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del forCalFeatureData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testPart['Card1VsTrans']=trainPart['card1'].map(cardVsTransAmtestLocal1Dict)\n",
    "# testPart['Card1VsTransStd']=trainPart['card1'].map(cardVsTransAmtestLocalStd1Dict)\n",
    "# testPart['Card4VsTrans']=trainPart['card4'].map(cardVsTransAmtestLocal4Dict)\n",
    "# testPart['Card4VsTransStd']=trainPart['card4'].map(cardVsTransAmtrainPart4Std4Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toUseCol=list(trainPart.columns)\n",
    "toUseCol.remove('isFraud')\n",
    "target='isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainPart[['card4','Card4VsTrans']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardVsTransAmtrainPart4Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData=lgb.Dataset(trainPart[toUseCol],trainPart[target])\n",
    "valData=lgb.Dataset(valPart[toUseCol],valPart[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "#         'metric':metrics,\n",
    "        'learning_rate': 0.01,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4,\n",
    "        'verbose': 0,\n",
    "        'metric':'auc'\n",
    "    }\n",
    "\n",
    "num_boost_round=8000\n",
    "early_stopping_rounds=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3 = lgb.train(lgb_params, \n",
    "                     trainData, \n",
    "                     valid_sets=[trainData, valData], \n",
    "                   valid_names=['train','valid'],\n",
    "#                      evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kakDict={k:j for k,j in zip(model3.feature_importance(),model3.feature_name())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[kakDict[i] for i in sorted(kakDict,reverse=True)][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kakDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble,metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain3=model3.predict(trainPart[toUseCol])\n",
    "scoreOftrain3=metrics.roc_auc_score(trainPart[target],predTrain3)\n",
    "valPred3=model3.predict(valPart[toUseCol])\n",
    "scoreOfVal3=metrics.roc_auc_score(valPart[target],valPred3)\n",
    "testPred3=model3.predict(testPart[toUseCol])\n",
    "scoreOfTest3=metrics.roc_auc_score(testPart[target],testPred3)\n",
    "print('ROC AUC for train {} and for validation {} for test {}'.format(scoreOftrain3,scoreOfVal3,scoreOfTest3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st ROC AUC for train 0.9517255925054339 and for validation 0.9114527600700442 for test 0.9018045350670539\n",
    "#2nd ROC AUC for train 0.9554808315356411 and for validation 0.9136923607565761 for test 0.9023918756643375\n",
    "#3rd ROC AUC for train 0.9540256532402678 and for validation 0.9124955852919424 for test 0.9021716726534883\n",
    "#4th ROC AUC for train 0.9519575151753169 and for validation 0.9111383915618384 for test 0.9025403905942307\n",
    "#5th ROC AUC for train 0.974518437878972 and for validation 0.922009759846402 for test 0.9049750781614286\n",
    "#6th ROC AUC for train 0.9730115102991304 and for validation 0.9222004317665984 for test 0.9040898320555035\n",
    "#7th ROC AUC for train 0.9763890969670054 and for validation 0.923294301337268 for test 0.905121158020004\n",
    "#8th ROC AUC for train 0.9829982642003638 and for validation 0.92727537802398 for test 0.896619898435757\n",
    "#9th ROC AUC for train 0.9851442813660034 and for validation 0.9268007129604092 for test 0.9041793505798761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviceInfoDef(brRec):\n",
    "    if str(brRec)=='nan':\n",
    "        return 'Other'\n",
    "    else:\n",
    "        if ('SAMSUNG' in brRec) or ('SM' in brRec) or ('GT' in brRec):\n",
    "            return 'SAMSUNG Device'\n",
    "        elif 'iOS' in brRec:\n",
    "            return 'iOS Device'\n",
    "        elif 'Windows' in brRec:\n",
    "            return 'Windows Device'\n",
    "        elif 'LG' in brRec:\n",
    "            return 'LG Device'\n",
    "        elif 'HUAWEI' in brRec:\n",
    "            return 'HUAWEI Device'\n",
    "        elif 'MacOS' in brRec:\n",
    "            return 'MacOS Device'\n",
    "#         elif 'Lenovo' in brRec:\n",
    "#             return 'Lenovo Device'\n",
    "#         elif 'Redmi' in brRec:\n",
    "#             return 'Redmi Device'\n",
    "        elif ('Moto' in brRec) or ('moto' in brRec):\n",
    "            return 'Moto Device'\n",
    "        elif ('Nexus' in brRec) or ('Pixel' in brRec):\n",
    "            return 'MacOS Device'\n",
    "#         elif 'HTC' in brRec:\n",
    "#             return 'HTC Device'\n",
    "#         elif 'Android' in brRec:\n",
    "#             return 'Android Device'\n",
    "#         elif 'Blade' in brRec:\n",
    "#             return 'Blade Device'\n",
    "#         elif ('Nokia' in brRec) or ('NOKIA' in brRec):\n",
    "#             return 'Nokia Device'\n",
    "#         elif ('ASUS' in brRec) or ('Asus' in brRec):\n",
    "#             return 'ASUS Device'\n",
    "        else:\n",
    "            return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedData['CountryDomain']=combinedData['P_emaildomain'].apply(lambda x: defGetCountryFromDomain(x))\n",
    "combinedData['R_emaildomain']=combinedData['R_emaildomain'].fillna('Other')\n",
    "combinedData['P_emaildomain']=combinedData['P_emaildomain'].fillna('Other')\n",
    "combinedData['DeviceInfo']=combinedData['DeviceInfo'].apply(lambda x: deviceInfoDef(x))\n",
    "combinedData['id_33']=combinedData['id_33'].apply(lambda x: screenReso(x))\n",
    "combinedData['id_31']=combinedData['id_31'].apply(lambda x: browserDef(x))\n",
    "combinedData['id_30']=combinedData['id_30'].apply(lambda x: deviceDef(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nP=np.percentile(combinedData['TransactionAmt'],99)\n",
    "combinedData['TransactionAmt']=combinedData['TransactionAmt'].apply(lambda x: nP if x >= nP else x)\n",
    "# sns.distplot(combinedData['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For C columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupC=['C'+str(i) for i in range(1,15)]\n",
    "from sklearn import decomposition,preprocessing\n",
    "pp1=preprocessing.StandardScaler()\n",
    "pp1.fit(combinedData[groupC])\n",
    "newGroupCdata=pp1.transform(combinedData[groupC])\n",
    "pcaGroupC.fit(newGroupCdata)\n",
    "pcaGroupC=decomposition.PCA()\n",
    "sum(pcaGroupC.explained_variance_ratio_[:2])\n",
    "pcaCColumns=['pcaC1','pcaC2']\n",
    "pcaForGroupC=pd.DataFrame(pcaGroupC.fit_transform(newGroupCdata)[::,:2],columns=pcaCColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For D columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupD=['D'+str(i) for i in range(1,16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in groupD:\n",
    "    print (len(pd.unique(combinedData[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in groupD:\n",
    "    combinedData[i]=combinedData[i].fillna(combinedData[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp2=preprocessing.StandardScaler()\n",
    "pp2.fit(combinedData[groupD])\n",
    "newGroupDdata=pp2.transform(combinedData[groupD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaGroupD=decomposition.PCA()\n",
    "pcaGroupD.fit(newGroupDdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pcaGroupD.explained_variance_ratio_[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaDColumns=['pcaD'+str(i) for i in range(1,10)]\n",
    "pcaForGroupD=pd.DataFrame(pcaGroupD.fit_transform(newGroupDdata)[::,:9],columns=pcaDColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Columns V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupV=['V'+str(i) for i in range(1,340)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in groupV:\n",
    "    print (len(pd.unique(combinedData[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in groupV:\n",
    "    combinedData[i]=combinedData[i].fillna(combinedData[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp3=preprocessing.StandardScaler()\n",
    "pp3.fit(combinedData[groupV])\n",
    "newGroupVdata=pp3.transform(combinedData[groupV])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaGroupV=decomposition.PCA()\n",
    "pcaGroupV.fit(newGroupVdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pcaGroupV.explained_variance_ratio_[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaVColumns=['pcaV'+str(i) for i in range(1,51)]\n",
    "pcaForGroupV=pd.DataFrame(pcaGroupD.fit_transform(newGroupVdata)[::,:50],columns=pcaVColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combinedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Columns for ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupID=['id_'+str(i).zfill(2) for i in range(1,12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in groupID:\n",
    "    print (len(pd.unique(combinedData[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in groupID:\n",
    "    combinedData[i]=combinedData[i].fillna(combinedData[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp4=preprocessing.StandardScaler()\n",
    "pp4.fit(combinedData[groupID])\n",
    "newGroupIDdata=pp4.transform(combinedData[groupID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaGroupID=decomposition.PCA()\n",
    "pcaGroupID.fit(newGroupIDdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pcaGroupID.explained_variance_ratio_[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaIDColumns=['pcaID'+str(i) for i in range(1,9)]\n",
    "pcaForGroupID=pd.DataFrame(pcaGroupID.fit_transform(newGroupIDdata)[::,:8],columns=pcaIDColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combinedData['id_26']=combinedData['id_26'].apply(lambda x: numDef(x))\n",
    "combinedData['id_25']=combinedData['id_25'].apply(lambda x: numDef25(x))\n",
    "combinedData['id_21']=combinedData['id_21'].apply(lambda x: numDef21(x))\n",
    "combinedData['id_20']=combinedData['id_20'].apply(lambda x: numDef21(x))\n",
    "combinedData['id_19']=combinedData['id_19'].apply(lambda x: numDef21(x))\n",
    "combinedData['id_17']=combinedData['id_17'].apply(lambda x: numDef(x))\n",
    "combinedData['id_13']=combinedData['id_13'].apply(lambda x: numDef13(x))\n",
    "\n",
    "combinedData['id_14']=combinedData['id_14'].apply(lambda x: numDef14(x))\n",
    "combinedData['id_22']=combinedData['id_22'].apply(lambda x: numDef22(x))\n",
    "combinedData['id_24']=combinedData['id_24'].apply(lambda x: numDef24(x))\n",
    "combinedData['id_18']=combinedData['id_18'].apply(lambda x: numDef18(x))\n",
    "combinedData['id_32']=combinedData['id_32'].apply(lambda x: numDef32(x))\n",
    "\n",
    "\n",
    "combinedData['card4']=combinedData['card4'].fillna('Other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedData['id_33']=combinedData['id_33'].apply(lambda x: screenReso(x))\n",
    "# combinedData['id_31']=combinedData['id_31'].apply(lambda x: browserDef(x))\n",
    "# combinedData['id_30']=combinedData['id_30'].apply(lambda x: deviceDef(x))\n",
    "# combinedData['id_26']=combinedData['id_26'].apply(lambda x: numDef(x))\n",
    "# combinedData['id_25']=combinedData['id_25'].apply(lambda x: numDef25(x))\n",
    "# combinedData['id_21']=combinedData['id_21'].apply(lambda x: numDef21(x))\n",
    "# combinedData['id_20']=combinedData['id_20'].apply(lambda x: numDef21(x))\n",
    "# combinedData['id_19']=combinedData['id_19'].apply(lambda x: numDef21(x))\n",
    "# combinedData['id_17']=combinedData['id_17'].apply(lambda x: numDef(x))\n",
    "# combinedData['id_13']=combinedData['id_13'].apply(lambda x: numDef13(x))\n",
    "# combinedData['DeviceInfo']=combinedData['DeviceInfo'].apply(lambda x: deviceInfoDef(x))\n",
    "# combinedData['id_14']=combinedData['id_14'].apply(lambda x: numDef14(x))\n",
    "# combinedData['id_22']=combinedData['id_22'].apply(lambda x: numDef22(x))\n",
    "# combinedData['id_24']=combinedData['id_24'].apply(lambda x: numDef24(x))\n",
    "# combinedData['id_18']=combinedData['id_18'].apply(lambda x: numDef18(x))\n",
    "# combinedData['id_32']=combinedData['id_32'].apply(lambda x: numDef32(x))\n",
    "\n",
    "# combinedData['CountryDomain']=combinedData['P_emaildomain'].apply(lambda x: defGetCountryFromDomain(x))\n",
    "# combinedData['R_emaildomain']=combinedData['R_emaildomain'].fillna('Other')\n",
    "# combinedData['P_emaildomain']=combinedData['P_emaildomain'].fillna('Other')\n",
    "# combinedData['card4']=combinedData['card4'].fillna('Other')\n",
    "# combinedData['card6']=combinedData['card6'].apply(lambda x: defCard6(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalCombinedData=['DeviceType','DeviceInfo']+['id_'+str(i) for i in range(12,39)]+\\\n",
    "                        ['M'+str(i) for i in range(1,10)]+\\\n",
    "                        ['ProductCD','P_emaildomain', 'R_emaildomain','CountryDomain','card4', 'card6',]#+\\\n",
    "                        #+['addr1','card1', 'card2', 'card3','card5', 'addr2']\n",
    "    \n",
    "leaveCols=['TransactionID', 'isFraud', 'TransactionDT']+\\\n",
    "            ['addr1','card1', 'card2', 'card3','card5', 'addr2']+\\\n",
    "            [ 'TransactionAmt',]\n",
    "# tonormalize=['addr1','card1', 'card2', 'card3','card5', 'addr2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in categoricalCombinedData:\n",
    "    print ('>>>>>>>>>>>',j.zfill(12),'>>>>',len(pd.unique(combinedData[j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# towork='id_18'\n",
    "# fig, ax = plt.subplots(figsize=(15,5))\n",
    "# sns.countplot(trainIdentity[towork].fillna(0))\n",
    "# # sns.countplot(trainIdentity['id_34'].fillna(0),ax=ax)\n",
    "\n",
    "# trainIdentity['id_18']=trainIdentity['id_18'].apply(lambda x: numDef18(x))\n",
    "# len(pd.unique(trainIdentity[towork]))\n",
    "# list(pd.unique(trainIdentity[towork]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cateDataTrainCombined=pd.get_dummies(combinedData[categoricalCombinedData],prefix_sep='_',drop_first='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toleaveTrainCombined=combinedData[leaveCols].fillna(-9999)\n",
    "cateDataTrainCombined=cateDataTrainCombined.fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proProcess1=preprocessing.MinMaxScaler()\n",
    "toleaveTrainCombined['addr1']=proProcess1.fit_transform(toleaveTrainCombined[['addr1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proProcess2=preprocessing.MinMaxScaler()\n",
    "toleaveTrainCombined['card1']=proProcess2.fit_transform(toleaveTrainCombined[['card1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proProcess3=preprocessing.MinMaxScaler()\n",
    "toleaveTrainCombined['card2']=proProcess3.fit_transform(toleaveTrainCombined[['card2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proProcess4=preprocessing.MinMaxScaler()\n",
    "toleaveTrainCombined['card3']=proProcess4.fit_transform(toleaveTrainCombined[['card3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proProcess5=preprocessing.MinMaxScaler()\n",
    "toleaveTrainCombined['card5']=proProcess5.fit_transform(toleaveTrainCombined[['card5']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proProcess6=preprocessing.MinMaxScaler()\n",
    "toleaveTrainCombined['addr2']=proProcess6.fit_transform(toleaveTrainCombined[['addr2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toleaveTrainCombined[['addr1','card1', 'card2', 'card3','card5', 'addr2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# towork='addr2'\n",
    "# # fig, ax = plt.subplots(figsize=(15,5))\n",
    "# # sns.countplot(trainTranscation[towork].fillna(0))\n",
    "# # sns.countplot(trainIdentity['id_34'].fillna(0),ax=ax)\n",
    "\n",
    "# trainTranscation['card6']=trainTranscation['card6'].apply(lambda x: defCard6(x))\n",
    "# len(pd.unique(trainTranscation[towork]))\n",
    "# list(pd.unique(trainTranscation[towork]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedTrainTransaction=pd.concat([toleaveTrainCombined,cateDataTrainCombined,\\\n",
    "                                     pcaForGroupID,pcaForGroupV,pcaForGroupD,pcaForGroupC,\\\n",
    "                                    combinedData[groupID],combinedData[groupV],\\\n",
    "                                     combinedData[groupC],combinedData[groupD]],axis=1)\n",
    "processedTrainTransaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempTimeSer=((processedTrainTransaction['TransactionDT']-86400)).map(int)\n",
    "processedTrainTransaction['transSec']=tempTimeSer%60\n",
    "processedTrainTransaction['transMin']=(tempTimeSer/60).map(int)%60\n",
    "processedTrainTransaction['transHour']=(tempTimeSer/3600).map(int)%24\n",
    "processedTrainTransaction['transMonth']=((tempTimeSer/86400)//30)+1\n",
    "processedTrainTransaction['transDay']=((tempTimeSer/86400)%30).map(int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del processedTrainTransaction['TransactionDT']\n",
    "del processedTrainTransaction['TransactionID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedTrainTransaction.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(processedTrainTransaction['isFraud'])\n",
    "# # sns.countplot(processedTrainTransaction['transMonth'],hue=processedTrainTransaction['isFraud'])\n",
    "\n",
    "# # sns.countplot(processedTrainTransaction['transMonth'])\n",
    "# sns.countplot(processedTrainTransaction['transMonth'],hue=processedTrainTransaction['isFraud'])\n",
    "\n",
    "# # sns.countplot(processedTrainTransaction['transDay'])\n",
    "# sns.countplot(processedTrainTransaction['transDay'],hue=processedTrainTransaction['isFraud'])\n",
    "\n",
    "# # sns.countplot(processedTrainTransaction['transHour'])\n",
    "# sns.countplot(processedTrainTransaction['transHour'],hue=processedTrainTransaction['isFraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(processedTrainTransaction['transMin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(processedTrainTransaction['transSec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=['isFraud']\n",
    "toUseCol=list(processedTrainTransaction.columns)\n",
    "toUseCol.remove(target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "Counter(toUseCol)\n",
    "\n",
    "len(toUseCol),len(set(toUseCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPart=processedTrainTransaction[processedTrainTransaction['transMonth']<=5]\n",
    "# valPart=processedTrainTransaction[processedTrainTransaction['transMonth']==5]\n",
    "testPart=processedTrainTransaction[processedTrainTransaction['transMonth']>5]\n",
    "trainPart.shape,testPart.shape#,valPart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble,metrics\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "X = trainPart[toUseCol]\n",
    "y = trainPart[target]\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "tscv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clfList=[]\n",
    "scoreList=[]\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "#     params = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "#                 'max_depth': 5, 'alpha': 10,'max_depth': 2}\n",
    "    \n",
    "#     clf = xgb.XGBClassifier(\n",
    "#         n_estimators=300, random_state=4,\n",
    "# #         tree_method='gpu_hist',\n",
    "#         **params\n",
    "#     )\n",
    "    \n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "    trainData=lgb.Dataset(X_train[toUseCol],y_train[target])\n",
    "    valData=lgb.Dataset(X_test[toUseCol],y_test[target])\n",
    "    lgb_params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "    #         'metric':metrics,\n",
    "            'learning_rate': 0.01,\n",
    "            #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "            'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "            'max_depth': -1,  # -1 means no limit\n",
    "            'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "            'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "            'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "            'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "            'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "            'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "            'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "            'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "            'reg_alpha': 0,  # L1 regularization term on weights\n",
    "            'reg_lambda': 0,  # L2 regularization term on weights\n",
    "            'nthread': 4,\n",
    "            'verbose': 0,\n",
    "            'metric':'auc'\n",
    "        }\n",
    "\n",
    "    num_boost_round=1000\n",
    "    early_stopping_rounds=10\n",
    "\n",
    "    model3 = lgb.train(lgb_params, \n",
    "                         trainData, \n",
    "                         valid_sets=[trainData, valData], \n",
    "                       valid_names=['train','valid'],\n",
    "    #                      evals_result=evals_results, \n",
    "                         num_boost_round=num_boost_round,\n",
    "                         early_stopping_rounds=early_stopping_rounds,\n",
    "                         verbose_eval=50)\n",
    "    \n",
    "    y_pred_train = model3.predict(X_test)#[:,1]\n",
    "    score = metrics.roc_auc_score(y_test, y_pred_train)\n",
    "    clfList.append(model3)\n",
    "    scoreList.append(y_pred_train)\n",
    "    print(f'ROC AUC {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testScoreList=[]\n",
    "for mo in clfList:\n",
    "    scr=mo.predict(testPart[toUseCol])\n",
    "    testScoreList.append(scr)\n",
    "    print (metrics.roc_auc_score( testPart[target],scr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTestNewMean=pd.DataFrame(testScoreList).transpose().mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score( testPart[target],scoreTestNewMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPart[toUseCol].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'objective': 'binary',\n",
    "# #         'metric':metrics,\n",
    "#         'learning_rate': 0.01,\n",
    "#         #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "#         'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "#         'max_depth': -1,  # -1 means no limit\n",
    "#         'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "#         'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "#         'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "#         'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "#         'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "#         'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "#         'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "#         'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "#         'reg_alpha': 0,  # L1 regularization term on weights\n",
    "#         'reg_lambda': 0,  # L2 regularization term on weights\n",
    "#         'nthread': 4,\n",
    "#         'verbose': 0,\n",
    "#         'metric':'auc'\n",
    "#     }\n",
    "\n",
    "# num_boost_round=3000\n",
    "# early_stopping_rounds=10\n",
    "\n",
    "# model3 = lgb.train(lgb_params, \n",
    "#                      trainData, \n",
    "#                      valid_sets=[trainData, valData], \n",
    "#                    valid_names=['train','valid'],\n",
    "# #                      evals_result=evals_results, \n",
    "#                      num_boost_round=num_boost_round,\n",
    "#                      early_stopping_rounds=early_stopping_rounds,\n",
    "#                      verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain3=bst.predict(trainPart[toUseCol])\n",
    "scoreOftrain3=metrics.roc_auc_score(trainPart[target],predTrain3)\n",
    "valPred3=bst.predict(valPart[toUseCol])\n",
    "scoreOfVal3=metrics.roc_auc_score(valPart[target],valPred3)\n",
    "testPred3=bst.predict(testPart[toUseCol])\n",
    "scoreOfTest3=metrics.roc_auc_score(testPart[target],testPred3)\n",
    "print('ROC AUC for train {} and for validation {} for test {}'.format(scoreOftrain3,scoreOfVal3,scoreOfTest3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUC for train 0.9543954336563735 and for validation 0.9114576197806739 for test 0.9040375652381945"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
